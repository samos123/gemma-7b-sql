{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-18 01:13:44--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
      "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.124, 18.172.134.88, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1710983624&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMDk4MzYyNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vimtCmvlTDWS-uNrNlZ59zxmTIrIoQiv4dpH3e5D2NG%7EWgaQ8t%7EaLWqHyCAT25cRtBR3Eblx%7EW0FdIu0d2zr-ItxBUK2dUBDRDpoMAaEb-a-qFgH5%7E1vCFLlvEHF0ae3SZw0tfHB%7EojuBd2N8GDY4RXc77RwYq7I-7JWCpxP4X55kMmvk-PNijUNcbX-%7EJqc%7E4na%7EWvkO-1MimHk6ZSkFgDjbrC8ByZIT%7EEmSM0jDlbkUPUxdd8q6CDg5oQjarE8aIa0nJjGkXxEsjeqOe6KaP%7EiLVBb-0iOFVBPD2y-JdqqcDavEvTBJdXcIatbHJwS3cRnV2U5jwu9jq-ZXwrgYA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-03-18 01:13:44--  https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1710983624&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMDk4MzYyNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vimtCmvlTDWS-uNrNlZ59zxmTIrIoQiv4dpH3e5D2NG%7EWgaQ8t%7EaLWqHyCAT25cRtBR3Eblx%7EW0FdIu0d2zr-ItxBUK2dUBDRDpoMAaEb-a-qFgH5%7E1vCFLlvEHF0ae3SZw0tfHB%7EojuBd2N8GDY4RXc77RwYq7I-7JWCpxP4X55kMmvk-PNijUNcbX-%7EJqc%7E4na%7EWvkO-1MimHk6ZSkFgDjbrC8ByZIT%7EEmSM0jDlbkUPUxdd8q6CDg5oQjarE8aIa0nJjGkXxEsjeqOe6KaP%7EiLVBb-0iOFVBPD2y-JdqqcDavEvTBJdXcIatbHJwS3cRnV2U5jwu9jq-ZXwrgYA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.94, 18.154.185.64, 18.154.185.27, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.94|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13085339 (12M) [text/plain]\n",
      "Saving to: ‘databricks-dolly-15k.jsonl’\n",
      "\n",
      "databricks-dolly-15 100%[===================>]  12.48M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-03-18 01:13:45 (100 MB/s) - ‘databricks-dolly-15k.jsonl’ saved [13085339/13085339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 78577\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"b-mc2/sql-create-context\")\n",
    "input_template = \"\"\"Generate valid SQL query for a given natural language query and schema.\n",
    "In your response only provide valid SQL for the schema, do not provide anything else.\n",
    "\n",
    "Natural language query:\n",
    "{question}\n",
    "\n",
    "Schema:\n",
    "```sql\n",
    "{schema}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "output_teplate = \"\"\"## RESPONSE\n",
    "```sql\n",
    "{answer}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def prompt(data):\n",
    "    input_text = input_template.format(question=data[\"question\"], schema=data[\"context\"])\n",
    "    output_text = output_teplate.format(answer=data[\"answer\"])\n",
    "    return {'input': input_text, \"output\": output_text}\n",
    "\n",
    "data = data.map(prompt).remove_columns([\"question\", \"answer\", \"context\"])\n",
    "data[\"train\"].to_json(\"nsql.jsonl\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nHow many heads of the departments are older than 56 ?\\n\\nSchema:\\n```sql\\nCREATE TABLE head (age INTEGER)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT COUNT(*) FROM head WHERE age > 56\\n```\\n\"}\n",
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nList the name, born state and age of the heads of departments ordered by age.\\n\\nSchema:\\n```sql\\nCREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT name, born_state, age FROM head ORDER BY age\\n```\\n\"}\n",
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nList the creation year, name and budget of each department.\\n\\nSchema:\\n```sql\\nCREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT creation, name, budget_in_billions FROM department\\n```\\n\"}\n",
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nWhat are the maximum and minimum budget of the departments?\\n\\nSchema:\\n```sql\\nCREATE TABLE department (budget_in_billions INTEGER)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT MAX(budget_in_billions), MIN(budget_in_billions) FROM department\\n```\\n\"}\n",
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nWhat is the average number of employees of the departments whose rank is between 10 and 15?\\n\\nSchema:\\n```sql\\nCREATE TABLE department (num_employees INTEGER, ranking INTEGER)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT AVG(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15\\n```\\n\"}\n",
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nWhat are the names of the heads who are born outside the California state?\\n\\nSchema:\\n```sql\\nCREATE TABLE head (name VARCHAR, born_state VARCHAR)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT name FROM head WHERE born_state <> 'California'\\n```\\n\"}\n",
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nWhat are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\\n\\nSchema:\\n```sql\\nCREATE TABLE department (creation VARCHAR, department_id VARCHAR); CREATE TABLE management (department_id VARCHAR, head_id VARCHAR); CREATE TABLE head (head_id VARCHAR, born_state VARCHAR)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT DISTINCT T1.creation FROM department AS T1 JOIN management AS T2 ON T1.department_id = T2.department_id JOIN head AS T3 ON T2.head_id = T3.head_id WHERE T3.born_state = 'Alabama'\\n```\\n\"}\n",
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nWhat are the names of the states where at least 3 heads were born?\\n\\nSchema:\\n```sql\\nCREATE TABLE head (born_state VARCHAR)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT born_state FROM head GROUP BY born_state HAVING COUNT(*) >= 3\\n```\\n\"}\n",
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nIn which year were most departments established?\\n\\nSchema:\\n```sql\\nCREATE TABLE department (creation VARCHAR)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT creation FROM department GROUP BY creation ORDER BY COUNT(*) DESC LIMIT 1\\n```\\n\"}\n",
      "{\"input\":\"Generate valid SQL query for a given natural language query and schema.\\nIn your response only provide valid SQL for the schema, do not provide anything else.\\n\\nNatural language query:\\nShow the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\\n\\nSchema:\\n```sql\\nCREATE TABLE management (department_id VARCHAR, temporary_acting VARCHAR); CREATE TABLE department (name VARCHAR, num_employees VARCHAR, department_id VARCHAR)\\n```\\n\",\"output\":\"## RESPONSE\\n```sql\\nSELECT T1.name, T1.num_employees FROM department AS T1 JOIN management AS T2 ON T1.department_id = T2.department_id WHERE T2.temporary_acting = 'Yes'\\n```\\n\"}\n"
     ]
    }
   ],
   "source": [
    "! head nsql.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 18 04:37:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             113W / 700W |  23197MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          Off | 00000000:05:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              69W / 700W |      7MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          Off | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              68W / 700W |      7MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          Off | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              68W / 700W |      7MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          Off | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              70W / 700W |      7MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          Off | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              68W / 700W |      7MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          Off | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              68W / 700W |      7MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          Off | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              71W / 700W |      7MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        78      C   /usr/bin/python                               0MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'NeMo-Aligner'...\n",
      "Note: switching to 'v0.2.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 3b8b70e update package info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///workspace/NeMo-Aligner\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: nemo-toolkit[nlp] in /usr/local/lib/python3.10/dist-packages (from nemo_aligner==0.2.0) (1.23.0rc0)\n",
      "Requirement already satisfied: nvidia-pytriton in /usr/local/lib/python3.10/dist-packages (from nemo_aligner==0.2.0) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.20.3)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.57.1+1.g1ff679645)\n",
      "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.24.4)\n",
      "Requirement already satisfied: onnx>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.15.0rc2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.8.2)\n",
      "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.18.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.2.0)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (68.2.2)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.9.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (4.62.3)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.1.0+6e4932c)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.16.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.34.46)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.7.0)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.7.4)\n",
      "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.9.2)\n",
      "Requirement already satisfied: flask-restful in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.3.10)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (6.1.1)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (5.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.10.0)\n",
      "Requirement already satisfied: ijson in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.2.3)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.42.1)\n",
      "Requirement already satisfied: markdown2 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.4.12)\n",
      "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.8.2)\n",
      "Collecting megatron-core==0.4.0 (from nemo-toolkit[nlp]->nemo_aligner==0.2.0)\n",
      "  Downloading megatron_core-0.4.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.8.1)\n",
      "Requirement already satisfied: opencc<1.1.7 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.1.6)\n",
      "Requirement already satisfied: pangu in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (4.0.6.1)\n",
      "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.6.1)\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.1.2)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.5.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.3.1)\n",
      "Requirement already satisfied: tensorstore<0.1.46 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.1.45)\n",
      "Requirement already satisfied: zarr in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.17.0)\n",
      "Collecting hydra-core<=1.3.2,>1.3 (from nemo-toolkit[nlp]->nemo_aligner==0.2.0)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: omegaconf<=2.3 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.2.3)\n",
      "Requirement already satisfied: pytorch-lightning<=2.0.7,>=2.0 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.0.7)\n",
      "Collecting torchmetrics>=0.11.0 (from nemo-toolkit[nlp]->nemo_aligner==0.2.0)\n",
      "  Downloading torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers>=4.36.0 (from nemo-toolkit[nlp]->nemo_aligner==0.2.0)\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.15.3)\n",
      "Collecting webdataset<=0.1.62,>=0.1.48 (from nemo-toolkit[nlp]->nemo_aligner==0.2.0)\n",
      "  Downloading webdataset-0.1.62-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.17.1)\n",
      "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (7.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.5.3)\n",
      "Requirement already satisfied: sacremoses>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.1.1)\n",
      "Requirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.2.0)\n",
      "Requirement already satisfied: youtokentome>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.0.6)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-pytriton->nemo_aligner==0.2.0) (4.24.4)\n",
      "Requirement already satisfied: pyzmq~=23.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-pytriton->nemo_aligner==0.2.0) (23.2.1)\n",
      "Requirement already satisfied: sh~=1.14 in /usr/local/lib/python3.10/dist-packages (from nvidia-pytriton->nemo_aligner==0.2.0) (1.14.3)\n",
      "Collecting tritonclient~=2.39 (from tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0)\n",
      "  Downloading tritonclient-2.43.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-inspect~=0.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-pytriton->nemo_aligner==0.2.0) (0.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (23.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core<=1.3.2,>1.3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (4.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.1.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2023.12.25)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.16.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=2.0.7,>=2.0->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.10.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.1.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.36.0->nemo-toolkit[nlp]->nemo_aligner==0.2.0)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.4.2)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from tritonclient~=2.39->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (1.14)\n",
      "Requirement already satisfied: urllib3>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from tritonclient~=2.39->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (2.2.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (1.60.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (3.9.1)\n",
      "Requirement already satisfied: geventhttpclient<=2.0.2,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (2.0.2)\n",
      "Requirement already satisfied: cuda-python in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (12.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect~=0.6.0->nvidia-pytriton->nemo_aligner==0.2.0) (1.0.0)\n",
      "Requirement already satisfied: braceexpand in /usr/local/lib/python3.10/dist-packages (from webdataset<=0.1.62,>=0.1.48->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.1.7)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.46 in /usr/local/lib/python3.10/dist-packages (from boto3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.34.46)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.70.16)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.8.0)\n",
      "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (9.0.1)\n",
      "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.0.2)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2023.3.post1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.2.13)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (4.12.3)\n",
      "Collecting pydantic>=1.9.1 (from inflect->nemo-toolkit[nlp]->nemo_aligner==0.2.0)\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.40.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.1.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.2.8)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.5.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.42.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.1.42)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (5.9.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.40.5)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.3.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.4.4)\n",
      "Requirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.3.3)\n",
      "Requirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.12.1)\n",
      "Requirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.19)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (4.0.3)\n",
      "Collecting urllib3>=2.0.7 (from tritonclient~=2.39->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0)\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.7.0)\n",
      "Requirement already satisfied: gevent>=0.13 in /usr/local/lib/python3.10/dist-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (24.2.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (2023.11.17)\n",
      "Requirement already satisfied: brotli in /usr/local/lib/python3.10/dist-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (1.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (4.0.11)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.1.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.6.0)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic>=1.9.1->inflect->nemo-toolkit[nlp]->nemo_aligner==0.2.0)\n",
      "  Downloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.3->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (2.5)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (3.0.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (1.3.0)\n",
      "Requirement already satisfied: zope.event in /usr/local/lib/python3.10/dist-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (5.0)\n",
      "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (6.2)\n",
      "Requirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]~=2.39->nvidia-pytriton->nemo_aligner==0.2.0) (3.0.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (5.0.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->nemo-toolkit[nlp]->nemo_aligner==0.2.0) (3.2.2)\n",
      "Downloading megatron_core-0.4.0-cp310-cp310-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tritonclient-2.43.0-py3-none-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading webdataset-0.1.62-py3-none-any.whl (32 kB)\n",
      "Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: nemo_aligner\n",
      "  Building editable for nemo_aligner (pyproject.toml): started\n",
      "  Building editable for nemo_aligner (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nemo_aligner: filename=nemo_aligner-0.2.0-0.editable-py3-none-any.whl size=8889 sha256=77540829bdbb23dd769816c4b75708ad6a191a86da12e71b699193bf503dd11a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0df581f7/wheels/f3/68/ed/b442ba6d13509144a5d260cc138f8df050ff4be2c8b33eab6d\n",
      "Successfully built nemo_aligner\n",
      "Installing collected packages: megatron-core, webdataset, urllib3, tensorboard-data-server, pydantic-core, tritonclient, pydantic, hydra-core, torchmetrics, tokenizers, transformers, nemo_aligner\n",
      "  Attempting uninstall: megatron-core\n",
      "    Found existing installation: megatron_core 0.5.0rc0\n",
      "    Uninstalling megatron_core-0.5.0rc0:\n",
      "      Successfully uninstalled megatron_core-0.5.0rc0\n",
      "  Attempting uninstall: webdataset\n",
      "    Found existing installation: webdataset 0.2.48\n",
      "    Uninstalling webdataset-0.2.48:\n",
      "      Successfully uninstalled webdataset-0.2.48\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: tritonclient\n",
      "    Found existing installation: tritonclient 2.38.0\n",
      "    Uninstalling tritonclient-2.38.0:\n",
      "      Successfully uninstalled tritonclient-2.38.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.8.2\n",
      "    Uninstalling pydantic-1.8.2:\n",
      "      Successfully uninstalled pydantic-1.8.2\n",
      "  Attempting uninstall: hydra-core\n",
      "    Found existing installation: hydra-core 1.2.0\n",
      "    Uninstalling hydra-core-1.2.0:\n",
      "      Successfully uninstalled hydra-core-1.2.0\n",
      "  Attempting uninstall: torchmetrics\n",
      "    Found existing installation: torchmetrics 0.9.1\n",
      "    Uninstalling torchmetrics-0.9.1:\n",
      "      Successfully uninstalled torchmetrics-0.9.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.33.1\n",
      "    Uninstalling transformers-4.33.1:\n",
      "      Successfully uninstalled transformers-4.33.1\n",
      "  Attempting uninstall: nemo_aligner\n",
      "    Found existing installation: nemo_aligner 0.1.0rc0\n",
      "    Uninstalling nemo_aligner-0.1.0rc0:\n",
      "      Successfully uninstalled nemo_aligner-0.1.0rc0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "img2dataset 1.45.0 requires wandb<0.17,>=0.16.0, but you have wandb 0.15.3 which is incompatible.\n",
      "img2dataset 1.45.0 requires webdataset<0.3,>=0.2.5, but you have webdataset 0.1.62 which is incompatible.\n",
      "rapids-dask-dependency 23.12.1 requires dask==2023.11.0, but you have dask 2023.9.2 which is incompatible.\n",
      "rapids-dask-dependency 23.12.1 requires distributed==2023.11.0, but you have distributed 2023.9.2 which is incompatible.\n",
      "spacy 3.1.3 requires pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4, but you have pydantic 2.6.4 which is incompatible.\n",
      "tb-nightly 2.17.0a20240220 requires tensorboard-data-server<0.8.0,>=0.7.0, but you have tensorboard-data-server 0.6.1 which is incompatible.\n",
      "tensorrt-llm 0.7.1 requires transformers==4.33.1, but you have transformers 4.38.2 which is incompatible.\n",
      "thinc 8.0.17 requires pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4, but you have pydantic 2.6.4 which is incompatible.\n",
      "ndc 0.1.0 requires spacy<4.0.0,>=3.6.0, but you have spacy 3.1.3 which is incompatible.\n",
      "ndc 0.1.0 requires sqlitedict==2.0.0, but you have sqlitedict 1.6.0 which is incompatible.\n",
      "ndc 0.1.0 requires zstandard==0.18.0, but you have zstandard 0.17.0 which is incompatible.\u001b[0m\u001b[31m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed hydra-core-1.3.2 megatron-core-0.4.0 nemo_aligner-0.2.0 pydantic-2.6.4 pydantic-core-2.16.3 tensorboard-data-server-0.6.1 tokenizers-0.15.2 torchmetrics-1.3.1 transformers-4.38.2 tritonclient-2.43.0 urllib3-2.0.7 webdataset-0.1.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/NVIDIA/NeMo-Aligner.git\n",
    "cd NeMo-Aligner && git checkout v0.2.0\n",
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-18 05:24:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-18 05:24:26 train_gpt_sft:118] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-03-18 05:24:26 train_gpt_sft:119] \n",
      "    name: gemma-7b-sql-nemo\n",
      "    trainer:\n",
      "      num_nodes: 1\n",
      "      devices: 8\n",
      "      accelerator: gpu\n",
      "      precision: bf16\n",
      "      sft:\n",
      "        max_epochs: 1\n",
      "        max_steps: -1\n",
      "        val_check_interval: 1000\n",
      "        save_interval: ${.val_check_interval}\n",
      "        limit_val_batches: 40\n",
      "        gradient_clip_val: 1.0\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_time: null\n",
      "      max_epochs: ${.sft.max_epochs}\n",
      "      max_steps: ${.sft.max_steps}\n",
      "    exp_manager:\n",
      "      explicit_log_dir: models/gemma-7b-sql-nemo\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_loss\n",
      "        save_top_k: 5\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: megatron_gpt_sft--{${.monitor}:.3f}-{step}-{consumed_samples}-{epoch}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 4\n",
      "      pipeline_model_parallel_size: 1\n",
      "      restore_from_path: /workspace/models/pytorch-7b-pt.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      encoder_seq_length: 4096\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      seq_len_interpolation_factor: null\n",
      "      use_flash_attention: null\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: none\n",
      "        restore_from_path: null\n",
      "        lora_tuning:\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "      data:\n",
      "        chat: false\n",
      "        chat_prompt_tokens:\n",
      "          system_turn_start: \"\\0\"\n",
      "          turn_start: \"\\x11\"\n",
      "          label_start: \"\\x12\"\n",
      "          end_of_turn: '\n",
      "    \n",
      "            '\n",
      "          end_of_name: '\n",
      "    \n",
      "            '\n",
      "        sample: false\n",
      "        num_workers: 0\n",
      "        dataloader_type: single\n",
      "        train_ds:\n",
      "          file_path: nsql.jsonl\n",
      "          global_batch_size: 128\n",
      "          micro_batch_size: 1\n",
      "          shuffle: true\n",
      "          memmap_workers: null\n",
      "          max_seq_length: 8192\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          hf_dataset: false\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_path: nsql.jsonl\n",
      "          global_batch_size: 128\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          max_seq_length: ${model.data.train_ds.max_seq_length}\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          hf_dataset: false\n",
      "          truncation_method: right\n",
      "          output_original_text: true\n",
      "      optim:\n",
      "        name: distributed_fused_adam\n",
      "        lr: 5.0e-06\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 10\n",
      "          constant_steps: 1000\n",
      "          min_lr: 9.0e-07\n",
      "      bias_activation_fusion: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-18 05:24:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-03-18 05:24:26 exp_manager:708] Exp_manager is logging to models/gemma-7b-sql-nemo, but it already exists.\n",
      "[NeMo W 2024-03-18 05:24:26 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :models/gemma-7b-sql-nemo/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-18 05:24:26 exp_manager:396] Experiments will be logged at models/gemma-7b-sql-nemo\n",
      "[NeMo I 2024-03-18 05:24:27 exp_manager:856] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-18 05:24:54 megatron_init:241] Rank 0 has data parallel group : [0, 4]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:247] Rank 0 has combined group of data parallel and context parallel : [0, 4]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:252] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:255] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:272] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:275] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:276] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:287] Rank 0 has model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:288] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:298] Rank 0 has tensor model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:302] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:303] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:317] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:329] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:335] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:336] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:337] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[NeMo I 2024-03-18 05:24:54 megatron_init:338] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-03-18 05:24:54 tokenizer_utils:191] Getting SentencePiece with model: /tmp/tmpjkayda0k/c1f49ba929c24b7e95b7219ca958f881_tokenizer-final.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-03-18 05:24:54 - PID:79654 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 64\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-18 05:24:54 megatron_base_model:520] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:1078] The model: GPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:492] The model: GPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:492] The model: GPTSFTModel() does not have field.name: bias_gelu_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:492] The model: GPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:54 megatron_base_model:492] The model: GPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-03-18 05:24:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:611: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
      "      warnings.warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "[NeMo I 2024-03-18 05:27:27 nlp_overrides:1100] Model GPTSFTModel was successfully restored from /workspace/models/pytorch-7b-pt.nemo.\n",
      "[NeMo I 2024-03-18 05:27:27 train_script_utils:169] Running full finetuning since no peft scheme is given.\n",
      "      | Name  | Type          | Params\n",
      "    ----------------------------------------\n",
      "    0 | model | Float16Module | 2.1 B \n",
      "    ----------------------------------------\n",
      "    2.1 B     Trainable params\n",
      "    0         Non-trainable params\n",
      "    2.1 B     Total params\n",
      "    8,538.206 Total estimated model params size (MB)\n",
      "[NeMo I 2024-03-18 05:27:27 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-03-18 05:27:27 text_memmap_dataset:525] Processing 1 data files using 104 workers\n",
      "[NeMo I 2024-03-18 05:27:29 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:01.612749\n",
      "[NeMo I 2024-03-18 05:27:30 text_memmap_dataset:525] Processing 1 data files using 104 workers\n",
      "[NeMo I 2024-03-18 05:27:31 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:01.441462\n",
      "[NeMo I 2024-03-18 05:27:31 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-03-18 05:27:31 text_memmap_dataset:249] Loading nsql.jsonl\n",
      "[NeMo I 2024-03-18 05:27:31 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000906\n",
      "[NeMo I 2024-03-18 05:27:31 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-03-18 05:27:31 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-03-18 05:27:31 text_memmap_dataset:525] Processing 1 data files using 104 workers\n",
      "[NeMo I 2024-03-18 05:27:33 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:01.411864\n",
      "[NeMo I 2024-03-18 05:27:33 text_memmap_dataset:525] Processing 1 data files using 104 workers\n",
      "[NeMo I 2024-03-18 05:27:34 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:01.369279\n",
      "[NeMo I 2024-03-18 05:27:34 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-03-18 05:27:34 text_memmap_dataset:249] Loading nsql.jsonl\n",
      "[NeMo I 2024-03-18 05:27:34 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000861\n",
      "[NeMo I 2024-03-18 05:27:34 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-03-18 05:27:34 builders:327] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-18 05:27:34 experimental:26] `<class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'>` is experimental and not ready for production yet. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-18 05:27:34 builders:327] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-18 05:27:34 experimental:26] `<class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'>` is experimental and not ready for production yet. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-18 05:27:40 megatron_gpt_model:1296] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 2.13e+09. Total number of model parameters: 8.54e+09.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-03-18 05:27:40 modelPT:723] Optimizer config = MegatronDistributedFusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 5e-06\n",
      "        weight_decay: 0.01\n",
      "    \n",
      "    Parameter Group 1\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 5e-06\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2024-03-18 05:27:40 lr_scheduler:915] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x793b24a42350>\" \n",
      "    will be used during training (effective maximum steps = 613) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 10\n",
      "    constant_steps: 1000\n",
      "    min_lr: 9.0e-07\n",
      "    max_steps: 613\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | Float16Module | 2.1 B \n",
      "----------------------------------------\n",
      "2.1 B     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 B     Total params\n",
      "8,538.206 Total estimated model params size (MB)\n",
      "Training steps:   4%|▎         | 22/613 [05:05<1:58:33, 12.04s/it, train_grad_norm=9.06, train_lr=9e-7, train_loss=0.113, train_consumed_samples=2816, train_step_time=6.93, train_epoch=1]  "
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export MODEL_PATH=models/gemma-7b-sql-nemo\n",
    "mkdir -p ${MODEL_PATH}\n",
    "\n",
    "python NeMo-Aligner/examples/nlp/gpt/train_gpt_sft.py \\\n",
    "   name=gemma-7b-sql-nemo \\\n",
    "   trainer.precision=bf16 \\\n",
    "   trainer.num_nodes=1 \\\n",
    "   trainer.devices=8 \\\n",
    "   trainer.sft.max_steps=-1 \\\n",
    "   trainer.sft.limit_val_batches=40 \\\n",
    "   trainer.sft.val_check_interval=1000 \\\n",
    "   model.tensor_model_parallel_size=4 \\\n",
    "   model.pipeline_model_parallel_size=1 \\\n",
    "   model.megatron_amp_O2=True \\\n",
    "   model.restore_from_path=/workspace/models/pytorch-7b-pt.nemo \\\n",
    "   model.optim.lr=5e-6 \\\n",
    "   model.answer_only_loss=True \\\n",
    "   ++model.bias_activation_fusion=true \\\n",
    "   model.data.num_workers=0 \\\n",
    "   model.data.train_ds.micro_batch_size=1 \\\n",
    "   model.data.train_ds.global_batch_size=128 \\\n",
    "   model.data.train_ds.max_seq_length=8192 \\\n",
    "   model.data.train_ds.file_path=nsql.jsonl \\\n",
    "   model.data.validation_ds.micro_batch_size=1 \\\n",
    "   model.data.validation_ds.global_batch_size=128 \\\n",
    "   model.data.validation_ds.drop_last=True \\\n",
    "   model.data.validation_ds.file_path=nsql.jsonl \\\n",
    "   exp_manager.explicit_log_dir=${MODEL_PATH} \\\n",
    "   exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "   exp_manager.resume_if_exists=True \\\n",
    "   exp_manager.resume_ignore_no_checkpoint=True \\\n",
    "   exp_manager.create_checkpoint_callback=True \\\n",
    "   exp_manager.checkpoint_callback_params.monitor=validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints\n",
      "cmd-args.log\n",
      "events.out.tfevents.1710737035.jupyter-pod.36463.0\n",
      "hparams.yaml\n",
      "lightning_logs.txt\n",
      "nemo_error_log.txt\n",
      "nemo_log_globalrank-0_localrank-0.txt\n",
      "nemo_log_globalrank-1_localrank-1.txt\n",
      "nemo_log_globalrank-2_localrank-2.txt\n",
      "nemo_log_globalrank-3_localrank-3.txt\n",
      "nemo_log_globalrank-4_localrank-4.txt\n",
      "nemo_log_globalrank-5_localrank-5.txt\n",
      "nemo_log_globalrank-6_localrank-6.txt\n",
      "nemo_log_globalrank-7_localrank-7.txt\n",
      "run_0\n"
     ]
    }
   ],
   "source": [
    "! ls models/gemma-7b-sql-nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p models/gemma-7b-sql-nemo-trt-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mpi lib /usr/local/lib/python3.10/dist-packages/mpi4py/MPI.cpython-310-x86_64-linux-gnu.so successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving weights: 100%|██████████| 169/169 [00:23<00:00,  7.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before engine building, CPU RAM Used (GB): 18.153247833251953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/18/2024-03:25:28] [TRT-LLM] [W] Invalid timing cache, using freshly created one\n",
      "[03/18/2024-03:25:28] [TRT-LLM] [I] Context FMHA Enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/18/2024-03:25:23] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 18275, GPU 3623 (MiB)\n",
      "[03/18/2024-03:25:28] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4397, GPU +1158, now: CPU 22808, GPU 4781 (MiB)\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/vocab_embedding/GATHER_0_output_0 and LMHeadModelBuilder/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/0/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/0/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/0/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/0/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/0/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/0/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/0/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/0/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/1/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/1/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/1/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/1/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/1/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/1/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/1/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/1/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/2/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/2/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/2/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/2/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/2/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/2/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/2/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/2/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/3/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/3/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/3/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/3/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/3/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/3/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/3/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/3/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/4/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/4/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/4/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/4/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/4/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/4/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/4/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/4/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/5/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/5/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/5/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/5/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/5/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/5/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/5/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/5/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/6/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/6/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/6/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/6/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/6/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/6/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/6/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/6/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/7/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/7/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/7/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/7/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/7/mlp/ELEMENTWISE_SUM_0_out"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/18/2024-03:25:28] [TRT-LLM] [I] Build TensorRT engine NeMo_bfloat16_tp1_rank0.engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/7/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/7/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/7/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/8/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/8/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/8/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/8/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/8/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/8/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/8/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/8/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/9/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/9/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/9/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/9/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/9/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/9/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/9/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/9/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/10/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/10/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/10/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/10/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/10/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/10/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/10/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/10/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/11/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/11/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/11/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/11/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/11/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/11/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/11/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/11/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/12/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/12/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/12/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/12/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/12/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/12/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/12/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/12/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/13/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/13/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/13/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/13/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/13/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/13/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/13/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/13/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/14/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/14/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/14/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/14/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/14/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/14/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/14/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/14/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/15/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/15/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/15/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/15/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/15/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/15/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/15/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/15/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/16/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/16/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/16/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/16/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/16/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/16/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/16/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/16/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/17/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/17/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/17/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/17/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/17/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/17/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/17/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/17/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/18/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/18/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/18/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/18/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/18/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/18/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/18/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/18/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/19/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/19/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/19/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/19/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/19/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/19/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/19/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/19/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/20/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/20/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/20/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/20/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/20/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/20/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/20/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/20/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/21/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/21/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/21/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/21/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/21/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/21/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/21/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/21/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/22/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/22/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/22/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/22/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/22/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/22/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/22/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/22/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/23/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/23/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/23/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/23/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/23/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/23/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/23/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/23/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/24/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/24/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/24/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/24/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/24/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/24/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/24/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/24/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/25/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/25/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/25/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/25/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/25/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/25/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/25/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/25/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/26/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/26/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/26/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/26/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/26/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/26/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/26/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/26/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/mlp/SHUFFLE_0_output_0 and LMHeadModelBuilder/layers/27/mlp/fc/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/27/mlp/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/mlp/SHUFFLE_2_output_0 and LMHeadModelBuilder/layers/27/mlp/ELEMENTWISE_POW_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/mlp/fc/PLUGIN_V2_Gemm_0_output_0 and LMHeadModelBuilder/layers/27/mlp/ELEMENTWISE_PROD_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/mlp/SHUFFLE_3_output_0 and LMHeadModelBuilder/layers/27/mlp/ELEMENTWISE_SUM_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/mlp/ELEMENTWISE_PROD_3_output_0 and LMHeadModelBuilder/layers/27/mlp/gate/PLUGIN_V2_Gemm_0_output_0: first input has type Float but second input has type BFloat16.\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/ELEMENTWISE_SUM_0_output_0 and LMHeadModelBuilder/layers/27/mlp/proj/PLUGIN_V2_Gemm_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/layers/27/ELEMENTWISE_SUM_1_output_0 and LMHeadModelBuilder/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] A shape layer can only run in INT32 precision\n",
      "[03/18/2024-03:25:28] [TRT] [W] IElementWiseLayer with inputs LMHeadModelBuilder/ln_f/REDUCE_AVG_0_output_0 and LMHeadModelBuilder/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
      "[03/18/2024-03:25:28] [TRT] [W] Unused Input: position_ids\n",
      "[03/18/2024-03:25:28] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[03/18/2024-03:25:28] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 22846, GPU 4919 (MiB)\n",
      "[03/18/2024-03:25:28] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +3, GPU +72, now: CPU 22849, GPU 4991 (MiB)\n",
      "[03/18/2024-03:25:28] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/18/2024-03:25:28] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[03/18/2024-03:25:43] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[03/18/2024-03:25:43] [TRT] [I] Detected 64 inputs and 29 output network tensors.\n",
      "[03/18/2024-03:25:50] [TRT] [I] Total Host Persistent Memory: 54896\n",
      "[03/18/2024-03:25:50] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[03/18/2024-03:25:50] [TRT] [I] Total Scratch Memory: 33562880\n",
      "[03/18/2024-03:25:50] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 569 steps to complete.\n",
      "[03/18/2024-03:25:50] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 20.3948ms to assign 11 blocks to 569 nodes requiring 729811456 bytes.\n",
      "[03/18/2024-03:25:50] [TRT] [I] Total Activation Memory: 729811456\n",
      "[03/18/2024-03:25:55] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[03/18/2024-03:25:55] [TRT] [I] Detected 64 inputs and 29 output network tensors.\n",
      "[03/18/2024-03:26:11] [TRT] [I] Total Host Persistent Memory: 54896\n",
      "[03/18/2024-03:26:11] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[03/18/2024-03:26:11] [TRT] [I] Total Scratch Memory: 33562880\n",
      "[03/18/2024-03:26:11] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 569 steps to complete.\n",
      "[03/18/2024-03:26:11] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 26.042ms to assign 11 blocks to 569 nodes requiring 38595584 bytes.\n",
      "[03/18/2024-03:26:11] [TRT] [I] Total Activation Memory: 38595584\n",
      "[03/18/2024-03:26:11] [TRT] [I] Total Weights Memory: 18648225792\n",
      "[03/18/2024-03:26:11] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 22916, GPU 22845 (MiB)\n",
      "[03/18/2024-03:26:11] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +72, now: CPU 22916, GPU 22917 (MiB)\n",
      "[03/18/2024-03:26:11] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/18/2024-03:26:11] [TRT] [I] Engine generation completed in 43.0364 seconds.\n",
      "[03/18/2024-03:26:11] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 3000 MiB, GPU 23784 MiB\n",
      "[03/18/2024-03:26:11] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +17785, now: CPU 0, GPU 17785 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/18/2024-03:26:20] [TRT-LLM] [I] Total time of building NeMo_bfloat16_tp1_rank0.engine: 00:00:51\n",
      "[03/18/2024-03:26:20] [TRT-LLM] [I] Config saved to models/gemma_dolly_finetuned_trt_llm/config.json.\n",
      "[03/18/2024-03:26:20] [TRT-LLM] [I] Serializing engine to models/gemma_dolly_finetuned_trt_llm/NeMo_bfloat16_tp1_rank0.engine...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/18/2024-03:26:20] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 41498 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/18/2024-03:26:41] [TRT-LLM] [I] Engine serialized. Total time: 00:00:21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/18/2024-03:26:41] [TRT] [I] Serialized 59 bytes of code generator cache.\n",
      "[03/18/2024-03:26:41] [TRT] [I] Serialized 551917 bytes of compilation cache.\n",
      "[03/18/2024-03:26:41] [TRT] [I] Serialized 49 timing cache entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/18/2024-03:26:41] [TRT-LLM] [I] Timing cache serialized to models/gemma_dolly_finetuned_trt_llm/model.cache\n",
      "[03/18/2024-03:26:42] [TRT-LLM] [I] Total time of building all 1 engines: 00:01:18\n",
      "[03/18/2024-03:26:43] [TRT-LLM] [I] Reading from serialize path models/gemma_dolly_finetuned_trt_llm/NeMo_bfloat16_tp1_rank0.engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Engine building rank 0, CPU RAM Used (GB): 23.14820098876953\n",
      "[03/18/2024-03:26:52] [TRT] [I] Loaded engine size: 17795 MiB\n",
      "[03/18/2024-03:26:54] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 36079, GPU 21579 (MiB)\n",
      "[03/18/2024-03:26:54] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 36079, GPU 21643 (MiB)\n",
      "[03/18/2024-03:26:54] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/18/2024-03:26:54] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +17784, now: CPU 0, GPU 17784 (MiB)\n",
      "[03/18/2024-03:26:54] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 36079, GPU 22277 (MiB)\n",
      "[03/18/2024-03:26:54] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 36079, GPU 22341 (MiB)\n",
      "[03/18/2024-03:26:54] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/18/2024-03:26:54] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 17784 (MiB)\n",
      "[03/18/2024-03:26:54] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 36194, GPU 22553 (MiB)\n",
      "[03/18/2024-03:26:54] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +70, now: CPU 36194, GPU 22623 (MiB)\n",
      "[03/18/2024-03:26:54] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/18/2024-03:26:55] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 17784 (MiB)\n",
      "[03/18/2024-03:26:55] [TRT] [I] Switching optimization profile from: 0 to 1. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n",
      "[03/18/2024-03:26:55] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 36309, GPU 22847 (MiB)\n",
      "[03/18/2024-03:26:55] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +72, now: CPU 36309, GPU 22919 (MiB)\n",
      "[03/18/2024-03:26:55] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.4\n",
      "[03/18/2024-03:26:56] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 17784 (MiB)\n",
      "[03/18/2024-03:26:56] [TRT] [I] Switching optimization profile from: 0 to 1. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n"
     ]
    }
   ],
   "source": [
    "from nemo.export import TensorRTLLM\n",
    "trt_llm_exporter = TensorRTLLM(model_dir=\"models/gemma-7b-sql-nemo-trt-llm\")\n",
    "trt_llm_exporter.export(nemo_checkpoint_path=\"models/gemma-7b-sql-nemo/checkpoints/gemma-7b-sql-nemo.nemo\", model_type=\"gemma\", n_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeMo_bfloat16_tp1_rank0.engine\tconfig.json  model.cache  tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "! ls models/models/gemma-7b-sql-nemo-trt-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-18 04:04:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/nested/__init__.py:165: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "      return _nested.nested_tensor(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[' teaming up to bring the cloud gaming service, NVIDIA GeForce NOW, to Android TV. The service will be available on Android TV devices from Sony, TCL, and Xiaomi.\\n\\nNVIDIA GeForce NOW is a cloud gaming service that allows you to play PC games on your Android TV. The service is powered by NVIDIA’s cloud']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_llm_exporter.forward([\"NVIDIA and Google are\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
